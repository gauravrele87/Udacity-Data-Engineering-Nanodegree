{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Data Warehouse\n",
    "\n",
    "### Introduction\n",
    "\n",
    "A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.\n",
    "\n",
    "As their data engineer, you are tasked with building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to. You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.\n",
    "\n",
    "\n",
    "### Project Description\n",
    "\n",
    "In this project, you'll apply what you've learned on data warehouses and AWS to build an ETL pipeline for a database hosted on Redshift. To complete the project, you will need to load data from S3 to staging tables on Redshift and execute SQL statements that create the analytics tables from these staging tables.\n",
    "\n",
    "Here are the steps you will take to create the schema for your tables and then copy teh data from Amazon S3 to Amazon Redshift. \n",
    "\n",
    "### Step 0: Create an Amazon Redshift Table\n",
    "\n",
    "Lets start with creating an Amazon Redshift table using the AWS CLI. You will need the AWS CLI installed for this to work and appropriate IAM permissions. If you are not sure what permissions you have, you can check your IAM user if you have IAM permissions or atleast type `aws configure` in the command line to check whether you have an access key and secret key.\n",
    "\n",
    "We are going to be using the cheapest Redshift cluster we could find. Through the docs/pricing page, `dc2.large` is the smallest and the cheapest that includes the query option.\n",
    "\n",
    "**Note**: If you can't access the cluster `connection refused` error then do the following\n",
    "1. click on your cluster name\n",
    "2. open the 'VPC security group' that your cluster belongs to\n",
    "3. At the bottom of the screen, click **Inbound**\n",
    "4. Click **Edit** > **Add new rule** and add a new **Redshift** rule with source **My IP**. DO NOT OPEN THE SG to all IP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "redshifts3_arn = iam.get_role(RoleName='RedshiftS3')['Role']['Arn']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash -s \"$redshifts3_arn\"\n",
    "#\n",
    "#aws redshift create-cluster --node-type dc2.large \\\n",
    "#                            --cluster-type single-node \\\n",
    "#                            --master-username admin \\\n",
    "#                            --master-user-password TopSecret1 \\\n",
    "#                            --cluster-identifier mycluster1 \\\n",
    "#                            --db-name dev \\\n",
    "#                            --iam-roles $1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Add all the information from your cluster into variables\n",
    "\n",
    "You can find most information above when you create the cluster. For the `host` name, check the redshift cluster in the AWS console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[CLUSTER]\n",
    "HOST= 'mycluster1.cyqgskzvawwc.us-west-2.redshift.amazonaws.com'\n",
    "DB_NAME= 'dev'\n",
    "DB_USER= 'admin'\n",
    "DB_PASSWORD= 'TopSecret1'\n",
    "DB_PORT= '5439'\n",
    "\n",
    "#[IAM_ROLE]\n",
    "ARN=redshifts3_arn\n",
    "\n",
    "#[S3]\n",
    "LOG_DATA='s3://udacity-dend/log_data'\n",
    "LOG_JSONPATH='s3://udacity-dend/log_json_path.json'\n",
    "SONG_DATA='s3://udacity-dend/song_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Design schemas for your fact and dimension tables.\n",
    "\n",
    "We will create multiple dimension tables and single fact table. The schema for the tables is given below:\n",
    "\n",
    "- Fact Table: `songplays` - records in event data associated with song plays i.e. records with page NextSong with variables `songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent`\n",
    "    \n",
    "- Dimension Tables: There are 4 dimension tables `users` table, `songs` table, `artists` table and `time` table.\n",
    "    - users - users in the app. Variables: `user_id, first_name, last_name, gender, level`.\n",
    "    - songs - songs in music database. Variables: `song_id, title, artist_id, year, duration`\n",
    "    - artists - artists in music database. Variables: `artist_id, name, location, lattitude, longitude`\n",
    "    - time - timestamps of records in songplays broken down into specific units. Variables: `start_time, hour, day, week, month, year, weekday`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# DROP TABLES\n",
    "\n",
    "staging_events_table_drop = \"DROP TABLE IF EXISTS staging_events\"\n",
    "staging_songs_table_drop = \"DROP TABLE IF EXISTS staging_songs\"\n",
    "songplay_table_drop = \"DROP TABLE IF EXISTS songplays\"\n",
    "user_table_drop = \"DROP TABLE IF EXISTS users\"\n",
    "song_table_drop = \"DROP TABLE IF EXISTS songs\"\n",
    "artist_table_drop = \"DROP TABLE IF EXISTS artists\"\n",
    "time_table_drop = \"DROP TABLE IF EXISTS time\"\n",
    "\n",
    "# CREATE TABLES\n",
    "\n",
    "\n",
    "staging_events_table_create= (\"\"\"\n",
    "\n",
    "CREATE TABLE staging_events\n",
    "(\n",
    "    artist            VARCHAR(255),\n",
    "    auth              VARCHAR(100),\n",
    "    firstName         TEXT,\n",
    "    gender            TEXT,\n",
    "    itemInSession     INTEGER,\n",
    "    lastName          TEXT,\n",
    "    length            FLOAT DEFAULT 0,\n",
    "    level             TEXT,\n",
    "    location          VARCHAR(50),\n",
    "    method            VARCHAR(50),\n",
    "    page              VARCHAR(50),\n",
    "    registration      REAL,\n",
    "    sessionId         INTEGER NOT NULL,\n",
    "    song              VARCHAR(255),\n",
    "    status            INTEGER,\n",
    "    ts                BIGINT,\n",
    "    userAgent         VARCHAR(255),\n",
    "    userId            INTEGER\n",
    ")\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "staging_songs_table_create = (\"\"\"\n",
    "\n",
    "CREATE TABLE staging_songs \n",
    "(\n",
    "  artist_id           TEXT,\n",
    "  artist_latitude     FLOAT,\n",
    "  artist_location     VARCHAR(255),\n",
    "  artist_longitude    FLOAT,\n",
    "  artist_name         VARCHAR(255),\n",
    "  duration            FLOAT,\n",
    "  num_songs           INTEGER,\n",
    "  song_id             VARCHAR(50),\n",
    "  title               VARCHAR(255),\n",
    "  year                INTEGER\n",
    ");\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "songplay_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS songplays (\n",
    "                                songplay_id bigint IDENTITY(0,1),\n",
    "                                start_time TIMESTAMP, \n",
    "                                user_id INT, \n",
    "                                level VARCHAR, \n",
    "                                song_id VARCHAR(64), \n",
    "                                artist_id VARCHAR(64), \n",
    "                                session_id INT, \n",
    "                                location TEXT, \n",
    "                                user_agent TEXT\n",
    "                                )\n",
    "\"\"\")\n",
    "\n",
    "user_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "                                user_id INT PRIMARY KEY, \n",
    "                                first_name VARCHAR(128), \n",
    "                                last_name VARCHAR(128), \n",
    "                                gender VARCHAR(5), \n",
    "                                level VARCHAR(5)\n",
    "                                )\n",
    "\"\"\")\n",
    "\n",
    "song_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS songs (\n",
    "                                song_id varchar PRIMARY KEY, \n",
    "                                title TEXT, \n",
    "                                artist_id VARCHAR(64), \n",
    "                                year INT, \n",
    "                                duration FLOAT(4)\n",
    "                                )\n",
    "\"\"\")\n",
    "\n",
    "artist_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS artists (\n",
    "                                artist_id varchar(64) PRIMARY KEY, \n",
    "                                name VARCHAR(255), \n",
    "                                location VARCHAR(255), \n",
    "                                lattitude FLOAT, \n",
    "                                longitude FLOAT\n",
    "                                )\n",
    "\"\"\")\n",
    "\n",
    "time_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS time (\n",
    "                                start_time TIMESTAMP primary key, \n",
    "                                hour INT, \n",
    "                                day INT, \n",
    "                                week INT, \n",
    "                                month INT, \n",
    "                                year INT, \n",
    "                                weekday TEXT\n",
    "                                )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Copying the raw Json data into a staging table in Redshift\n",
    "\n",
    "You will start with the raw json files in the s3 bucket `udacity-dend` and you will copy the data into the respective redshift tables `staging_events` and `staging_songs`. Once you are done with copying the data, you will use this data to insert the data into the respective dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGING TABLES\n",
    "\n",
    "staging_events_copy = (\"\"\"\n",
    "copy staging_events\n",
    "from 's3://udacity-dend/log_data'\n",
    "iam_role '{}'\n",
    "FORMAT AS json '{}'\n",
    "\"\"\").format(ARN,LOG_JSONPATH)\n",
    "\n",
    "staging_songs_copy = (\"\"\"\n",
    "copy staging_songs\n",
    "from 's3://udacity-dend/song_data'\n",
    "iam_role '{}'\n",
    "BLANKSASNULL\n",
    "EMPTYASNULL\n",
    "FORMAT AS json 'auto'\n",
    "\"\"\").format(ARN)\n",
    "\n",
    "# FINAL TABLES\n",
    "\n",
    "songplay_table_insert = (\"\"\"\n",
    "INSERT into songplays (start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)\n",
    "(select \n",
    "  TIMESTAMP 'epoch' + e.ts/1000 * interval '1 second', \n",
    "  e.userId, \n",
    "  e.level, \n",
    "  (select s.song_id from staging_songs as s where s.title = e.song and s.artist_name = e.artist) as song_id,\n",
    "  (select s.artist_id from staging_songs as s where s.title = e.song and s.artist_name = e.artist) as artist_id,\n",
    "  e.sessionId,\n",
    "  e.location,\n",
    "  e.userAgent\n",
    "from staging_events as e\n",
    "where e.page = 'NextSong');\n",
    "\"\"\")\n",
    "\n",
    "user_table_insert = (\"\"\"\n",
    "INSERT into users\n",
    "(select DISTINCT\n",
    "  userId,\n",
    "  firstName,\n",
    "  lastName,\n",
    "  gender,\n",
    "  level\n",
    "FROM staging_events\n",
    "WHERE page = 'NextSong'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "song_table_insert = (\"\"\"\n",
    "INSERT into songs \n",
    "(SELECT DISTINCT\n",
    "    song_id\n",
    "    title,\n",
    "    artist_id,\n",
    "    year,\n",
    "    duration\n",
    "FROM staging_songs\n",
    "\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "artist_table_insert = (\"\"\"\n",
    "INSERT into artists \n",
    "(SELECT DISTINCT\n",
    "    artist_id,\n",
    "    artist_name,\n",
    "    artist_location,\n",
    "    artist_latitude,\n",
    "    artist_longitude\n",
    "FROM staging_songs\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "time_table_insert = (\"\"\"\n",
    "INSERT into time \n",
    "(Select \n",
    "    distinct t_start_time,\n",
    "    EXTRACT(HOUR FROM t_start_time) As t_hour,\n",
    "    EXTRACT(DAY FROM t_start_time) As t_day,\n",
    "    EXTRACT(WEEK FROM t_start_time) As t_week,\n",
    "    EXTRACT(MONTH FROM t_start_time) As t_month,\n",
    "    EXTRACT(YEAR FROM t_start_time) As t_year,\n",
    "    EXTRACT(DOW FROM t_start_time) As t_weekday\n",
    "FROM (\n",
    "SELECT distinct TIMESTAMP 'epoch' + ts/1000 * interval '1 second' as t_start_time\n",
    "FROM staging_events \n",
    "WHERE page = 'NextSong'\n",
    ")\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# QUERY LISTS\n",
    "\n",
    "create_table_queries = [staging_events_table_create, staging_songs_table_create, songplay_table_create, user_table_create, song_table_create, artist_table_create, time_table_create]\n",
    "drop_table_queries = [staging_events_table_drop, staging_songs_table_drop, songplay_table_drop, user_table_drop, song_table_drop, artist_table_drop, time_table_drop]\n",
    "copy_table_queries = [staging_events_copy, staging_songs_copy]\n",
    "insert_table_queries = [songplay_table_insert, user_table_insert, song_table_insert, artist_table_insert, time_table_insert]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the tables in the redshfit cluster (Create_tables.py)\n",
    "\n",
    "Now you will connect to the redshift cluster and you will use the queries above to create the tables in the redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import configparser\n",
    "import psycopg2\n",
    "#from sql_queries import create_table_queries, drop_table_queries\n",
    "\n",
    "\n",
    "def drop_tables(cur, conn):\n",
    "    for query in drop_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def create_tables(cur, conn):\n",
    "    for query in create_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def create_tables_main():\n",
    "    #config = configparser.ConfigParser()\n",
    "    #config.read('dwh.cfg')\n",
    "\n",
    "    conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(HOST, DB_NAME, DB_USER, DB_PASSWORD, DB_PORT))\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    drop_tables(cur, conn)\n",
    "    create_tables(cur, conn)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "create_tables_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the tables in the redshfit cluster (etl.py)\n",
    "\n",
    "Now you will connect to the redshift cluster and you will use the queries above to insert the rows from the staging tables to the fact and dimension tables. You can use another connector to connect to the cluster. This might help in debugging error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(HOST, DB_NAME, DB_USER, DB_PASSWORD, DB_PORT))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import configparser\n",
    "import psycopg2\n",
    "#from sql_queries import copy_table_queries, insert_table_queries\n",
    "\n",
    "\n",
    "def load_staging_tables(cur, conn):\n",
    "    for i,query in enumerate(copy_table_queries):\n",
    "        print(f'Query {i+1}/{len(copy_table_queries)} for staging tables')\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "        print('Done')\n",
    "\n",
    "\n",
    "def insert_tables(cur, conn):\n",
    "    for i,query in enumerate(insert_table_queries):\n",
    "        print(f'Query {i+1}/{len(insert_table_queries)} for inserting tables')\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "        print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1/2 for staging tables\n",
      "Done\n",
      "Query 2/2 for staging tables\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "load_staging_tables(cur, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1/5 for inserting tables\n",
      "Done\n",
      "Query 2/5 for inserting tables\n",
      "Done\n",
      "Query 3/5 for inserting tables\n",
      "Done\n",
      "Query 4/5 for inserting tables\n",
      "Done\n",
      "Query 5/5 for inserting tables\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#conn.rollback()\n",
    "insert_tables(cur, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
