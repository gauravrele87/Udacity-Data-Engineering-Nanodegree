{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='font-size:50px'>Project: Data Lake</h1>\n",
    "\n",
    "# Introduction\n",
    "\n",
    "A music streaming startup, Sparkify, has grown their user base and song database even more and want to move their data warehouse to a data lake. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.\n",
    "\n",
    "As their data engineer, you are tasked with building an ETL pipeline that extracts their data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables. This will allow their analytics team to continue finding insights in what songs their users are listening to.\n",
    "\n",
    "You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.\n",
    "\n",
    "# Project Description\n",
    "\n",
    "In this project, you'll apply what you've learned on Spark and data lakes to build an ETL pipeline for a data lake hosted on S3. To complete the project, you will need to load data from S3, process the data into analytics tables using Spark, and load them back into S3. You'll deploy this Spark process on a cluster using AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema for Song Play Analysis\n",
    "\n",
    "Using the song and log datasets, you'll need to create a star schema optimized for queries on song play analysis. This includes the following tables.\n",
    "\n",
    "### Fact Table\n",
    "\n",
    "- **songplays** - records in log data associated with song plays i.e. records with page `NextSong` with table schema\n",
    "    - songplay_id\n",
    "    - start_time\n",
    "    - user_id\n",
    "    - level\n",
    "    - song_id\n",
    "    - artist_id\n",
    "    - session_id\n",
    "    - location\n",
    "    - user_agent\n",
    "\n",
    "### Dimension Tables\n",
    "\n",
    "- **users** - users in the app\n",
    "    - user_id\n",
    "    - first_name\n",
    "    - last_name\n",
    "    - gender \n",
    "    - level\n",
    "    \n",
    "- **songs** - songs in music database\n",
    "    - song_id\n",
    "    - title\n",
    "    - artist_id\n",
    "    - year\n",
    "    - duration\n",
    "\n",
    "- **artists** - artists in music database\n",
    "    - artist_id\n",
    "    - name\n",
    "    - location\n",
    "    - lattitude\n",
    "    - longitude\n",
    "    \n",
    "- **time** - timestamps of records in songplays broken down into specific units\n",
    "    - start_time\n",
    "    - hour\n",
    "    - day\n",
    "    - week\n",
    "    - month\n",
    "    - year\n",
    "    - weekday\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Create the EMR cluster\n",
    "\n",
    "Creating an EMR cluster is a pain using boto3 \n",
    "Create the EMR cluster using the console with the following options\n",
    "\n",
    "- Launch mode: cluster ... Use the execution mode when you complete the scripting and development\n",
    "- Release: 5.26.0\n",
    "- Applications: Spark: Spark 2.4.3 on Hadoop 2.8.5 YARN with Ganglia 3.7.2 and Zeppelin 0.8.1\n",
    "- Instance type: m3.xlarge ... using smaller instances for dev\n",
    "- number of instances: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import dependencies and create a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1567298058798_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-3-101.us-west-2.compute.internal:20888/proxy/application_1567298058798_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-10-175.us-west-2.compute.internal:8042/node/containerlogs/container_1567298058798_0001_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "\n",
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step II: Creation Functions to process the song data and log data\n",
    "\n",
    "The two functions will process the song and log data and save the output in an Amazon S3 bucket that the engineer can specify. These datasets can be used by downstream applications to create a dashboard, or by data scientists to be used in a machine learning training job using Amazon SageMaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Song Data\n",
      "Reading in song data\n",
      "DataFrame[artist_id: string, artist_latitude: double, artist_location: string, artist_longitude: double, artist_name: string, duration: double, num_songs: bigint, song_id: string, title: string, year: bigint]\n",
      "Writing in song table\n",
      "Extracting artists table\n",
      "Writing in artists table\n",
      "Processing Log Data\n",
      "Reading in log data\n",
      "Extracting artists table data\n",
      "Writing in users table\n",
      "Creating time table\n",
      "Creating songplays table"
     ]
    }
   ],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    This function can be used to read the files stored in Amazon S3 (/song_data)\n",
    "    to get the information for the tables songs, artists\n",
    "    The required variables in the tables are\n",
    "    Song Table: unique song id (song_id) which is the primary key, the title (title) of the song, \n",
    "                the artist id (artist_id) for the artist, \n",
    "                year when the song was produced (year) and duration of the song (duration).\n",
    "    Artist Table: unique artist id (artist_id) which is the primary key, name of the artist (artist_name), \n",
    "                  location (location), latitude of the artist location (artist_latitude), longitude of \n",
    "                  artists location (artist_longitude).\n",
    "    \n",
    "    Arguments:\n",
    "        spark: the spark session you want to use. \n",
    "        input_data: Amazon S3 path for the raw data\n",
    "        output_data: Amazon S3 path for storing the processed data\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # get filepath to song data file\n",
    "    song_data = input_data + 'song_data/*/*/*/*.json'\n",
    "    \n",
    "    # read song data file\n",
    "    print('Reading in song data')\n",
    "    df = spark.read.json(song_data)\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df[['song_id', 'title', 'artist_id', 'year', 'duration']]\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    print('Writing in song table')\n",
    "    songs_table.write.parquet(output_data + \"/songs_table\")\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    print('Extracting artists table')\n",
    "    artists_table = df[['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']]\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    print('Writing in artists table')\n",
    "    artists_table.write.parquet(output_data+'/artists_table')\n",
    "\n",
    "\n",
    "def process_log_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    This function is used to read the files stored in Amazon S3 (data/log_data)\n",
    "    to get the user and time info and used to populate the users and time dim tables.\n",
    "                 \n",
    "    The required variables for the users table and time dim tables are\n",
    "    Users Table: Unique user id (user_id), first name of the user (first_name), last name (last_name), gender (gender)\n",
    "                 and subscrption level (level)\n",
    "                 \n",
    "    Time dimenension table: time('TimeStamp', 'Hour', 'Day', 'Week', 'Month', 'Year', 'Weekday')\n",
    "    \n",
    "    Arguments:\n",
    "        spark: the spark session you want to use. \n",
    "        input_data: Amazon S3 path for the raw data\n",
    "        output_data: Amazon S3 path for storing the processed data\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # get filepath to log data file\n",
    "    log_data = input_data + 'log_data/*/*/'\n",
    "\n",
    "    # read log data file\n",
    "    print('Reading in log data')\n",
    "    df = spark.read.json(log_data)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df[df['page'] == 'NextSong']\n",
    "\n",
    "    # extract columns for users table   \n",
    "    print('Extracting artists table data')\n",
    "    artists_table = df[['userId', 'firstName', 'lastName', 'gender', 'level']]\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    print('Writing in users table')\n",
    "    artists_table.write.parquet(output_data+'/users_table')\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    print('Creating time table')\n",
    "    get_timestamp = udf(lambda x: datetime.fromtimestamp(x / 1000.0), TimestampType())\n",
    "    df = df.withColumn('start_time', get_timestamp(df.ts))\n",
    "    \n",
    "    # create datetime column from original timestamp column\n",
    "    get_datetime = udf(lambda x: datetime.fromtimestamp(x / 1000.0), TimestampType())\n",
    "    df = df.withColumn('datetime', get_datetime(df.ts))\n",
    "    df = df.withColumn('hour', F.hour(df.datetime))\n",
    "    df = df.withColumn('day', F.dayofmonth(df.datetime))\n",
    "    df = df.withColumn('week', F.weekofyear(df.datetime))\n",
    "    df = df.withColumn('month', F.month(df.datetime))\n",
    "    df = df.withColumn('year', F.year(df.datetime))\n",
    "    df = df.withColumn('weekday', F.dayofweek(df.datetime))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = df[['start_time', 'hour', 'day', 'week', 'month', 'year', 'weekday']]\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.parquet(output_data+'/timetable')\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    print('Creating songplays table')\n",
    "    song_df = spark.read.json(input_data + 'song_data/*/*/*/*.json')\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    songplays_table = df.join(song_df, song_df.artist_name == df.artist)\n",
    "    songplays_table = songplays_table.withColumn(\"songplay_id\",F.monotonically_increasing_id())\n",
    "    songplays_table = songplays_table[['songplay_id', 'start_time', 'userId', 'level', 'song_id', \n",
    "                                      'artist_id', 'sessionId', 'location', 'userAgent']]\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.parquet(output_data + '/songplays_table')\n",
    "\n",
    "\n",
    "\n",
    "spark = create_spark_session()\n",
    "input_data = \"s3://udacity-dend/\"\n",
    "#input_data = \"s3://randombucketgaurav/udacity-data/\"\n",
    "output_data = \"s3://randombucketgaurav/udacity-data/output/\"\n",
    "\n",
    "print('Processing Song Data')\n",
    "process_song_data(spark, input_data, output_data)    \n",
    "print('Processing Log Data')\n",
    "process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
